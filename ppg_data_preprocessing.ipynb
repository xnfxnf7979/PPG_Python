{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9p3_0HGUXvBx",
    "outputId": "74e57d8a-8452-4af2-895b-03056640b5d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive._mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHY16MQRiaF1"
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow-gpu\n",
    "\n",
    "# load the required library\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from google.colab import drive\n",
    "drive._mount('/content/drive')\n",
    "\n",
    "# which deep learning model to use\n",
    "model_choice = 'ANN' # RNN or BIRNN\n",
    "\n",
    "# model parameters \n",
    "timesteps = 60\n",
    "sampling_frequency = 40\n",
    "hidden_layer_units = 10\n",
    "\n",
    "# create checkpoint path for the model\n",
    "if model_choice == 'ANN':\n",
    "  FILEWRITER_PATH = '파일경로/ppg_dae_ANN' + str(timesteps) + 'timesteps_' + str(hidden_layer_units) + 'notes_tensorboard'\n",
    "  if not os.path.isdir(FILEWRITER_PATH):\n",
    "    os.makedirs(FILEWRITER_PATH)\n",
    "  CHECKPOINT_PATH = '파일경로/ppg_dae_ANN' + str(timesteps) + 'timesteps_' + str(hidden_layer_units) + 'notes_tensorboard/checkpoints'\n",
    "  if not os.path.isdir(CHECKPOINT_PATH):\n",
    "    os.makedirs(CHECKPOINT_PATH)\n",
    "\n",
    "elif model_choice == 'RNN':\n",
    "  FILEWRITER_PATH = '파일경로/ppg_dae_RNN' + str(timesteps) + 'timesteps_' + str(hidden_layer_units) + 'notes_tensorboard'\n",
    "  if not os.path.isdir(FILEWRITER_PATH):\n",
    "    os.makedirs(FILEWRITER_PATH)\n",
    "  CHECKPOINT_PATH = '파일경로/ppg_dae_RNN' + str(timesteps) + 'timesteps_' + str(hidden_layer_units) + 'notes_tensorboard/checkpoints'\n",
    "  if not os.path.isdir(CHECKPOINT_PATH):\n",
    "    os.makedirs(CHECKPOINT_PATH) \n",
    "\n",
    "elif model_choice == 'BIRNN':\n",
    "  FILEWRITER_PATH = '파일경로/ppg_dae_BIRNN' + str(timesteps) + 'timesteps_' + str(hidden_layer_units) + 'notes_tensorboard'\n",
    "  if not os.path.isdir(FILEWRITER_PATH):\n",
    "    os.makedirs(FILEWRITER_PATH)\n",
    "  CHECKPOINT_PATH = '파일경로/ppg_dae_BIRNN' + str(timesteps) + 'timesteps_' + str(hidden_layer_units) + 'notes_tensorboard/checkpoints'\n",
    "  if not os.path.isdir(CHECKPOINT_PATH):\n",
    "    os.makedirs(CHECKPOINT_PATH) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5GHAz7Oaru6"
   },
   "outputs": [],
   "source": [
    "# -- deep learning definition --- #\n",
    "\n",
    "def ANN():\n",
    "  model_inputs = tf.keras.input(shape=(timesteps, ))\n",
    "  encoding_layer = tf.keras.layers.Dense(utils=hidden_units, activation='sigmoid', input_shape=(timesteps, ))(model_inputs)\n",
    "  out_layer = tf.keras.layers.Dense(units=timesteps, activation = 'sigmoid')(encoding_layer)\n",
    "\n",
    "  model = tf.keras.Model(inputs=model_inputs, outputs=out_layer)\n",
    "\n",
    "  return model\n",
    "\n",
    "def RNN():\n",
    "  model_inputs = tf.keras.input(shape=(timesteps, 1))\n",
    "  # ppg 인풋 1초가 100샘플 : 0.01 초, 0.02초, .. 이런식으로 타임스탬프 순으로 입력되므로 차원 1개 더 추가된 것\n",
    "  encoding_layer = tf.keras.layers.LSTM(units=hidden_units, activation='sigmoid',input_shape=(timesteps, 1))(model_inputs)\n",
    "  # return_sequence : 각 타임스탬프에서 결과 나올거냐/마지막에 나올거냐에 따라 True/False\n",
    "  # sigmoid : 0~1 결과값일 경우 시그모이드, 오토엔코더 쓸땐 대부분 시그모이드 씀 tanh는 0~-1값 없으므로 안쓴것\n",
    "  out_layer = tf.keras.layers.Dense(units=timesteps, activation = 'sigmoid')(encoding_layer)\n",
    "\n",
    "  model = tf.keras.Model(inputs=model_inputs, outputs=out_layer)\n",
    "  \n",
    "  return model\n",
    "\n",
    "def BIRNN():\n",
    "  model_inputs = tf.keras.input(shape=(timesteps, 1))\n",
    "  encoding_layer = tf.keras.layers.Bidrectional(tf.keras.layers.LSTM(units=hidden_units, activation='sigmoid',input_shape=(timesteps, 1)))(model_inputs)\n",
    "  out_layer = tf.keras.layers.Dense(units=timesteps, activation = 'sigmoid')(encoding_layer)\n",
    "\n",
    "  model = tf.keras.Model(inputs=model_inputs, outputs=out_layer)\n",
    "\n",
    "  return model\n",
    "\n",
    "# many-to-many model\n",
    "def BIRNN_MM():\n",
    "  model_inputs = tf.keras.input(shape=(timesteps, 1))\n",
    "  encoding_layer = tf.keras.layers.Bidrectional(tf.keras.layers.LSTM(units=hidden_units, activation='sigmoid',input_shape=(timesteps, 1)), return_sequence=True)(model_inputs)\n",
    "  # True > 타임스탭마다 hidden layer activation unit 만큼 데이터 나오게 됨, hidden units * timestemps 이므로 두 차원, out layer도 두 디멘션으로 나오게 됨   \n",
    "  out_layer = tf.keras.layers.Dense(units=1, activation = 'sigmoid')(encoding_layer)\n",
    "  # hidden units 개수대로 나오고 그게 outlayer 1개로 이어지는 구조라 unit=1이 됨\n",
    "\n",
    "  model = tf.keras.Model(inputs=model_inputs, outputs=out_layer)\n",
    "\n",
    "  return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MryAGcAz-GxD"
   },
   "outputs": [],
   "source": [
    "# -- load data & pre=processing -- #\n",
    "\n",
    "# set this the path data file\n",
    "ppg_data_path = '/content/drive/MyDrive/filepath'\n",
    "\n",
    "def signal_extraction(data_path):\n",
    "  # [0] index > dataset에 파일 하나뿐이기 때문, 파일이 더 많을 경우 for문으로 각 파일 로드해야함\n",
    "  ppg_data = np.loadtext(data_path)\n",
    "  print('the shape of the original dataset is %i by %i'% np.shape(ppg_data)) # (25000,400) -> 10000개의, 40Hz * 10sec ppg data\n",
    "  print('the shape of the %i timestep dataset is %i by %i'%(timesteps, int(np.cell((400-timesteps)/timesteps)*len(ppg_data)), timesteps))\n",
    "\n",
    "  # shuffle data\n",
    "  shuffle(ppg_data)\n",
    "\n",
    "  # create emtpy lists to hold training and validation datasets\n",
    "  train_input_data_list = []\n",
    "  train_output_data_list =[]\n",
    "  val_input_data_list =[]\n",
    "  val_output_data_list = []\n",
    "\n",
    "  # loop through the original dataset and perform data extraction/noise addition on each row\n",
    "  for datanum in range(len(ppg_data)):\n",
    "    # seperate the ppg_data segment in each row into timestep size\n",
    "    for index in range(0,400-timesteps, timesteps):\n",
    "      # range > 0~340, 60씩 \n",
    "\n",
    "      # --- original data, normalized -- #\n",
    "      a = ppg_data[datanum, index: index+timesteps].copy()\n",
    "      a= a-np.min(a)\n",
    "      a = a/np.max(a)\n",
    "      plt.figure()\n",
    "      # plt.plot(a)\n",
    "      # plt.show()\n",
    "\n",
    "      # -- gaussian noise data -- #\n",
    "      noise1 = np.random.randn(len(a))*1/3\n",
    "      b = a.copy() + noise1\n",
    "      b = b - np.min(b)\n",
    "      b = b / np.max(b)\n",
    "      plt.figure()\n",
    "      # plt.plot(b-1)\n",
    "      # plt.show()\n",
    "\n",
    "      # -- gaussian + low freq noise data -- #\n",
    "      c = b.copy()\n",
    "      slope = np.random.randn()\n",
    "      for i in range(len(c)):\n",
    "        c[i] = c[i] + slope/len(c)*i\n",
    "      c = c - np.min(c)\n",
    "      c = c/np.max(c)\n",
    "      # plt.figure()\n",
    "      # plt.plot(c)\n",
    "\n",
    "      # -- g + l + saturation data -- #\n",
    "      d = c.copy()\n",
    "      location1 = np.random.randint(timesteps, size=1)\n",
    "      location2 = location1 + np.random.randint(timesteps-location1)\n",
    "      if np.random.randn() > 0:\n",
    "        d[location1: location2] = np.zeros(location2-location1, float) # np.ones()도 가능,\n",
    "      else:\n",
    "        d[location1: location2] = np.ones(location2-location1, float)\n",
    "\n",
    "      # put 80% of the data into the training dataset list, and 20% into the vla dataset list\n",
    "      if datanum < 0.8 * len(ppg_data):\n",
    "        train_input_data_list.append(a) # change this to the noise-augmented data\n",
    "        train_output_data_list.appnd(a)\n",
    "      else:\n",
    "        val_input_data_list.append(a) # change this to the noise-augmentation data\n",
    "        val_output_data_list.append(a)\n",
    "\n",
    "  # plot last data\n",
    "  plt.figure()\n",
    "  plt.plot(ppg_data[datanum])\n",
    "  plt.plot(ppg_data[datanum,index:index+timesteps])\n",
    "  plt.show()\n",
    "  plt.figure()\n",
    "  plt.plot(a)\n",
    "  plt.plot(b)\n",
    "  plt.show()\n",
    "  plt.figure()\n",
    "  plt.plot(a)\n",
    "  plt.plot(c)\n",
    "  plt.show()\n",
    "  plt.figure()\n",
    "  plt.plot(a)\n",
    "  plt.plot(d)\n",
    "  plt.show()\n",
    "\n",
    "  # convert the dataset lists into arrays\n",
    "  train_input_data_list = np.asarray(train_input_data_list)\n",
    "  train_output_data_list = np.asarray(train_output_data_list)\n",
    "  val_input_data_list = np.asarray(val_input_data_list)\n",
    "  val_output_data_list = np.asarray(val_output_data_list)\n",
    "\n",
    "  return train_input_data_list, train_output_data_list, val_input_data_list, val_output_data_list\n",
    "\n",
    "[train_input_data_list, train_answer_data_list, val_input_data_list, val_answer_data_list] = signal_extraction(ppg_data_path)\n",
    "\n",
    "# if the deep learning model used here is recurrent. need to reshape the input data to match the keras lstm input shape\n",
    "if model_choice == 'RNN' or model_choice == 'BIRNN':\n",
    "  train_input_data_list = np.reshape(train_input_data_list, [len(train_input_data_list),timesteps,1])\n",
    "  val_input_data_list = np.reshape(val_input_data_list, [len(val_input_data_list),timesteps, 1])\n",
    "\n",
    "print(np.shape(train_input_data_list))\n",
    "print(np.shape(train_answer_data_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUepOITSUGYi"
   },
   "outputs": [],
   "source": [
    "# -- deep learning model training & validation -- #\n",
    "if model_choice == 'ANN':\n",
    "  DAE = ANN()\n",
    "elif model_choice == 'RNN':\n",
    "  DAE = RNN()\n",
    "elif model_choice == 'BIRNN':\n",
    "  DAE = BIRNN()\n",
    "\n",
    "# compile the model with optimizer and loss function\n",
    "# opt = tf.keras.optimizers.Adam(learning_rate =1)\n",
    "DAE.compile(optimizer='adam', loss='mse')\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(CHECKPOINT_PATH, verbose=1, save_best_only=True, save_waights_only=True, period=1)\n",
    "\n",
    "# train the model \n",
    "DAE.fit(train_input_data_list, train_answer_data_list, epochs=10000, batch_size=40000, shuffle=True, validation_data=(val_input_data_list, val_answer_data_list),\n",
    "        verbose=2, callbacks=[cp.callbacks])\n",
    "\n",
    "# load the trained model\n",
    "DAE.load_weights(CHECKPOINT_PATH)\n",
    "\n",
    "# generate predictions inferences based on validation input data\n",
    "prediction = DAE.predict(val_input_data_list)\n",
    "\n",
    "# resize the validation input data, prediction, and the answer for saving purposes\n",
    "prediction = np.reshape(prediction, [-1])\n",
    "val_input_data_list = np.reshape(val_input_data_list, [-1])\n",
    "val_answer_data_list = np.reshape(val_answer_data_list, [-1])\n",
    "\n",
    "# save 10% of the validation results for further analyses\n",
    "name = '/content/drive...' + model_choice + '_' + str(timesteps) + 'timesteps_' + str(hidden_layer_units) + 'notes_prediction_results.txt'\n",
    "file = open(name, 'w')\n",
    "for result_num in range(int(len(prediction)/10)):\n",
    "  file.write(\"%f %f %f \\n\" % (val_input_data_list[result_num], predictions[result_num], val_answer_data_list[result_num]))\n",
    "file.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(val_input_data_list[0: timesteps])\n",
    "plt.plot(prediction[0:timesteps])\n",
    "plt.plot(val_answer_data_list[0:timesteps])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ppg_data_preprocessing",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
